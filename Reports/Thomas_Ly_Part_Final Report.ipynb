{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c8b278f-dc83-49ff-89a5-9cfe2a556c52",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b5c2c-be4b-4277-b1e8-2a66a3ac9529",
   "metadata": {},
   "source": [
    "##### Facial recognition has long remained one of Data Science's most difficult areas to approach. Whereas other types of data have easy to define features and relative simplicity, facial data includes a great deal of hidden or noisy information. Due to this, facial recognition remains a daunting field with no single approach guaranteed to achieve the desired result. And although the human brain excels at instinctively deriving difficult to define features at a glance, programs still struggle to extract something as basic as gender. Yet, should a model be developed that performs as well as humans in all conditions, it would  vastly increase efficiency in all sorts of fields. Basic examples include, medical diagnoses based on facial features, the removal of the need for identification documents, the increased ease of entering one's favorite sites etc. In light of this, the task was to perform exploratory analysis on a number of preprocessing techniques, combined with an analysis of the best performing, and the best performing hyperparameter for said models. Out of four preprocessing techniques (Label Balancing, SIFT,  PCA, RFS) we determined that Label Balancing with oversampling was the best for generalization, while the other techniques lowered training time in exchange for a far greater error rate. Out of four models explored (GBC, CNN, RFS, SVM), GBC and CNN were chosen for similar levels of high accuracy in addition to differing training methods. We then determined the best hyperparameters for each model and visualized how each model functioned at peak performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92bb15c-b0ad-435f-aa75-718f3c92aa66",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd292b33-4723-4367-a556-b5218b80eb75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94e83faf-25d3-4264-ad29-9abd02605dbd",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec95d81-c70d-42ca-be3b-ba29099794dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5643b37b-1ae1-451c-ae62-3578780a7c40",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b406d025-5d35-4b05-a236-df5e906223a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10b4960e-d64f-4db8-a235-434e89217e31",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf824c9-17b8-4e76-99c3-af4ae3571ccf",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db428b67-d2a2-4ae8-8866-93c9b7552211",
   "metadata": {},
   "source": [
    "### Balance via Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f7c49d-b1d6-415e-b6e9-d95be3537303",
   "metadata": {},
   "source": [
    "### SIFT - (Scale Invariant Feature Transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ef4ba-7018-4b24-a721-0fed720d6280",
   "metadata": {},
   "source": [
    "##### A technique for simplifying the complexity of an image by transforming it into a histogram of commonly found features. The features within an image are defined as keypoints within the SIFT algorithm. A key point is defined as a local extrema within an image that is found by comparing a pixel with its neighbors for drastic shifts in pixel values. Next, a descriptor is taken of the local area around each key point which consists of a 128 bin feature vector. This vector describes the local area and a direction, allowing the keypoint to be applicable despite rotation. 128 bin descriptors are collected from training images and clustered via K-means to produce common descriptors. Then each image can be transformed into a histogram with each bin representing the number of times a common feature was detected within the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581b0b6-2fee-402c-a391-9e601a578730",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd42a83-89da-4961-9135-b18924c323e0",
   "metadata": {},
   "source": [
    "### RFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313a5888-20b0-4fd0-8e78-f9773171a567",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e96c4-45ca-4b28-a48e-02a42d6de817",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedc8945-67ff-4af1-ab9d-fcf80866fe9f",
   "metadata": {},
   "source": [
    "##### Each neural network was run with a maximum of 10 epochs with the optimizer adam and sparse categorical cross entropy loss. A callback was implemented with a patience of 5 and monitored the validation accuracy. This was so that the model would return the weights for the best validation accuracy should the model run for 5 epochs without improvement. Testing will be done on the age variable due to it having the most unbalanced and varied classes.\n",
    "##### Metrics used will be base accuracy, macro f1-score, macro recall, and macro-precision. This is so that we may compare how the model is doing on the entire validation dataset as well as whether it has equal metrics across all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921e537-ea9b-4111-a241-58bb0db76f21",
   "metadata": {},
   "source": [
    "#### Preprocessing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580c748d-51bb-4b60-a024-10bb19c51cf3",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr width = \"200\">\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/PreProcessingResults.PNG\"/>\n",
    "            <figcaption>Fig.14 Results matrix for preprocessing on validation</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4d500-5604-42e9-9536-f138cac6fc7d",
   "metadata": {},
   "source": [
    "#### Balanced versus Unbalanced\n",
    "##### While the unbalanced dataset had greater accuracy than the balanced dataset, the balanced dataset had superior macro precision, recall, and f1-score. Balanced datasets would be chosen from then on.\n",
    "#### Normalized versus Raw data\n",
    "##### The normalized dataset had worse results in addition to requiring additional memory to store float64 values instead of int8. Raw data should be chosen from then on.\n",
    "#### Balanced versus RFS, PCA, SIFT\n",
    "##### All metrics resulting from the preprocessing techniques were worse than corresponding metrics in the non preprocessed balanced dataset. Thus, no preprocessing techniques would be utilized.\n",
    "#### Verdict:\n",
    "##### The non-normalized, balanced, non-preprocessed dataset has the best performance out of all iterations with an accuracy of 0.42."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c1855-2a5f-4cf5-b58c-391f32cb6963",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0f85b4-2840-4d79-8804-42d6e3bdd88b",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b19a5b-11f8-466d-889e-0ed64be91070",
   "metadata": {},
   "source": [
    "##### After concluding that a balanced dataset yielded the best performance and that the model itself was the 2nd most accurate, further hyperparameter tuning was needed to achieve a global maximum of performance. Five hyperparameters were tuned for the following runs: Dropout, L1 Regularization, L2 Regularization, Learning Rate, and Number of samples. A hyperparameter would only be considered if its accuracy was greater than the base model by more than 0.01. This threshold is to ensure that improvements are not due to changes in samplinga and are instead because the model genuinely improved.\n",
    "#### Base Model's Hyperparameters\n",
    "##### - Dropout: 0\n",
    "##### - L1: 0\n",
    "##### - L2: 0\n",
    "##### - Lr: 0.001\n",
    "##### - Sampling Size: 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8d8fdd-40ee-4ff4-9341-308396ed426e",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a776a856-530f-4d7e-99c5-ed53b5a7e157",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr width = \"2000\">\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/L2Report.PNG\"/>\n",
    "            <figcaption>Fig.15 L2 hyperparameter tuning values</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/L1Report.PNG\" />\n",
    "            <figcaption>Fig.16 L1 hyperparameter tuning values</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/DropoutReport.PNG\" />\n",
    "            <figcaption>Fig.17 Dropout hyperparameter tuning values</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/LrReport.PNG\" />\n",
    "            <figcaption>Fig.18 Learning Rate hyperparameter tuning values</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c173d55-6c09-4b9d-a511-fc45b9982efc",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Based on hyperparameter tuning on all models, L1: 0.001 and Lr: 0.0001 were the best candidates since their accuracy was beyond 0.01 of the base accuracy of 0.485552."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de283cd5-625e-4299-9f5c-5d95cf8ef344",
   "metadata": {},
   "source": [
    "### Sample Size Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae34cb2-d236-4dec-b7c8-9fefe0ddff5c",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr width = \"2000\">\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/SampleAcc.PNG\"/>\n",
    "            <figcaption>Fig.27 Sample Size hyperparameter tuning versus accuracy</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/SampleLoss.PNG\" />\n",
    "            <figcaption>Fig.28 Sample Size hyperparameter tuning versus loss</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7034d4b3-ecf3-4f8e-aafe-c01e4e41fc1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Sample size amounts were tested from 500 to 8000 in intervals of 500. It was found that 5000 had the highest accuracy. Like before, the model loss seems to inversely correlate with the model's accuracy. Although the model was most accurate on 5000 samples, we will be using the 8000 as using more data is best practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d02c06-5e96-4710-8142-152ff0b445ac",
   "metadata": {},
   "source": [
    "### Post Hyperparameter testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a767e205-f7bd-4338-9007-f015be78043e",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr width = \"2000\">\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/8000TestRaceReport.PNG\"/>\n",
    "            <figcaption>Fig.30 Classification report for the race test set</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/8000TestReport.PNG\" />\n",
    "            <figcaption>Fig.31 Classification report on the age test set</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/8000TestGenderReport.PNG\" />\n",
    "            <figcaption>Fig.32 Classification report on the gender test set</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ede95-34dd-444a-90ea-990d438c38b3",
   "metadata": {},
   "source": [
    "##### After some basic runs, it was discovered that LR: 0.0001 had the best validation accuracy at 0.61 and a test accuracy of 0.62. The classification report on age shows far better results than that of the control model. In addition, the race and gender model using the same neural network structure achieved better results than the dummy models. Thus, this model was to be considered satisfactory. The only thing left to do was to visualize the weakpoints of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3450dfaa-d9c4-44f2-8398-bb6bbcc54c0b",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dcf296-a336-4bc7-8a11-061e4129b37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d56a646-0ef1-4041-bb90-66d86613f4f4",
   "metadata": {},
   "source": [
    "# Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1a5b3-de40-4302-8501-3182bc7dcce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afc89917-1a55-414d-961f-9b0cf365f88a",
   "metadata": {},
   "source": [
    "# Bibliography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bb54d4-d684-437b-b17d-5f5da5249f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80c029bf-33fa-4643-8a92-a7e73c528f5f",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2490961-3d9a-40a7-a5cb-a2d913372784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
