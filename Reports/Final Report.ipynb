{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c8b278f-dc83-49ff-89a5-9cfe2a556c52",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b5c2c-be4b-4277-b1e8-2a66a3ac9529",
   "metadata": {},
   "source": [
    "##### Facial recognition has long remained one of Data Science's most difficult areas to approach. Whereas other types of data have easy to define features and relative simplicity, facial data includes a great deal of hidden or noisy information. Due to this, facial recognition remains a daunting field with no single approach guaranteed to achieve the desired result. And although the human brain excels at instinctively deriving difficult to define features at a glance, programs still struggle to extract something as basic as gender. Yet, should a model be developed that performs as well as humans in all conditions, it would  vastly increase efficiency in all sorts of fields. Basic examples include, medical diagnoses based on facial features, the removal of the need for identification documents, the increased ease of entering one's favorite sites etc. In light of this, the task was to perform exploratory analysis on a number of preprocessing techniques, combined with an analysis of the best performing, and the best performing hyperparameter for said models. Out of four preprocessing techniques (Label Balancing, SIFT,  PCA, RFS) we determined that Label Balancing with oversampling was the best for generalization, while the other techniques lowered training time in exchange for a far greater error rate. Out of four models explored (GBC, CNN, RFS, SVM), GBC and CNN were chosen for similar levels of high accuracy in addition to differing training methods. We then determined the best hyperparameters for each model and visualized how each model functioned at peak performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92bb15c-b0ad-435f-aa75-718f3c92aa66",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf151e-8f0c-4547-8e45-c1646affad6f",
   "metadata": {},
   "source": [
    "Image classification techniques are widely apply in the machine learning community and well understood by many. Today's world rely more and more on advance features to protect our identities online and to protect many sensitive information such as SSN and banck accounts. While basic features, such as eye color, nose, etc. are easiily recognisable to the models there are many others that remain still abstract and not easily recognized by the algorithms. Some of this features include age, gender and race.Many models attempt to solve this problems but the criteria and actual performance are still inadequate. Adding these features to the basic features could potencially increase the level of security in face recognision applications. The combinations of unique physical features combined with other variables like age, gender and race could add an extra layer of security protection to any application that needs this for sensible online transaction and for our right to privacy while online. \n",
    "<br>\n",
    "<br>\n",
    "Based on previous workd done on the field it is clear that there is no one way to solve this problem. There is a diversity of algorithms apply to solve this problem as well as a diversity of benchmarks to asses test performace. In this project the focus was on two particular models, Gradient Boosting and Neural Networks. Moreover, an emphasis was applied to data engineering and various pre-processing techniques.\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e83faf-25d3-4264-ad29-9abd02605dbd",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec95d81-c70d-42ca-be3b-ba29099794dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5643b37b-1ae1-451c-ae62-3578780a7c40",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f89905-f82a-4181-b353-7750ea736896",
   "metadata": {},
   "source": [
    "##### The Dataset was taken from Kaggle and could be uploaded in this link. The Faces dataset contains 20000+ cropped & aligned facial images with age, race and gender labels. Age label modified to 9 stages based on The Stages  of Human Life Cycle. Project has 6000 testing examples 3500 validation and 10500 training. After balancing training examples, we took 5000 samples proportionally. The pixel values are integer between 0 and 255. To normalize data, we divide it by 255. Each image has a shape of (200,200,3). Feature extracted by PCA.\n",
    "The UTKFACE data set consist of 20000 labeled images. The images's labels consist of the target variables of age, gender and race. The age in the set ranges from 0-116 years old. The gender is 0 for make and 1 for female, and race from 0-4.\n",
    "<ul>\n",
    "    <li>[age] is an integer from 0 to 116, indicating the age</li>\n",
    "    <li>[gender] is either 0 (male) or 1 (female)</li>\n",
    "<li>[race] is an integer from 0 to 4, denoting White, Black, Asian, Indian, and Others (like Hispanic, Latino, Middle Eastern).</li>\n",
    " </ul>\n",
    " \n",
    "The data set consits of mainly two categories, \"in the wild image set\" which are pictures of people with different backgrounds and settings and \"cropped images\" which are images were the face of each person was cropped to exclued as much of the bacground as possible. In this project the latter was used to train and test the models.\n",
    "\n",
    "[ADD IMAGE INBALANCE]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b4960e-d64f-4db8-a235-434e89217e31",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8fba0-d71f-412c-ac82-92cd00ba45ac",
   "metadata": {},
   "source": [
    "#### Balancing Labels\n",
    "Balanced labels provide as more accuracy prediction. Unbalanced labels could be a case of undersampling or oversampling as well as incorrect  minority class classification could be a cause of huge issues.\n",
    "On the pic. 1 and 2 you can see how labels looks before and after balancing. Late Childhood (Ages 9-11)has the least samples. Class Early Adulthood (Ages 21-35) and v has the most samples. To implement the balancing we use the SMOTE Technique. The main idea of the SMOTE algorithm is to analyze the minority samples and artificially synthesize new samples based on the minority samples to add to the dataset the flow of the algorithm is as follows:\n",
    " 1. For each sample x in the minority class, use the Euclidean distance as a standard to calculate the distance from it to all samples in the sample set of the minority class to get its k nearest neighbors.\n",
    "\n",
    " 2. Set the sampling factor to determine the sample increase N according to the sample imbalance factor. For each minority sample x, randomly select several samples from the k nearest neighbors, assuming that the chosen nearest neighbor is xn.\n",
    "\n",
    " 3. For each randomly chosen neighbor xn, create a new sample with the original sample according to the following formula. During the balancing the classes move to (-1) stage.\n",
    "\n",
    "\n",
    "  - #1 infancy (0-2)                                    #0 infancy (0-2)\n",
    "  - #2 Early Childhood (Ages 3-5)                       #1 Early Childhood (Ages 3-5)\n",
    "  - #3 Middle Childhood (Ages 6-8)                      #2 Middle Childhood (Ages 6-8\n",
    "  - #4 Late Childhood (Ages 9-11)                       #3 Late Childhood (Ages 9-11)\n",
    "  - #5 Adolescence (Ages 12-20)                         #4 Adolescence (Ages 12-20)\n",
    "  - #6 Early Adulthood (Ages 21-35)                     #5 Early Adulthood (Ages 21-35)\n",
    "  - #7 Midlife (Ages 36-50)                             #6 Midlife (Ages 36-50)\n",
    "  - #8 Mature Adulthood (Ages 51-79)                    #7 Mature Adulthood (Ages 51-79)\n",
    "  - #9 Late Adulthood (Age 80+)                         #8 Late Adulthood (Age 80+)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb4fa5-a1b2-422b-a444-ec6e19db8bf0",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e87542a-888f-4287-978b-03592898e9f4",
   "metadata": {},
   "source": [
    "##### Each neural network was run with a maximum of 10 epochs with the optimizer adam and sparse categorical cross entropy loss. A callback was implemented with a patience of 5 and monitored the validation accuracy. This was so that the model would return the weights for the best validation accuracy should the model run for 5 epochs without improvement. Testing will be done on the age variable due to it having the most unbalanced and varied classes.\n",
    "##### Metrics used will be base accuracy, macro f1-score, macro recall, and macro-precision. This is so that we may compare how the model is doing on the entire validation dataset as well as whether it has equal metrics across all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808975b1-154f-4f79-b944-83d1b3980746",
   "metadata": {},
   "source": [
    "#### Preprocessing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8705163b-9ac8-49c5-b51b-a11526e9804c",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr width = \"200\">\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/PreProcessingResults.PNG\"/>\n",
    "            <figcaption>Fig.14 Results matrix for preprocessing on validation</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2f56c-924e-4ab9-aade-12ebc3166dd6",
   "metadata": {},
   "source": [
    "#### Balanced versus Unbalanced\n",
    "##### While the unbalanced dataset had greater accuracy than the balanced dataset, the balanced dataset had superior macro precision, recall, and f1-score. Balanced datasets would be chosen from then on.\n",
    "#### Normalized versus Raw data\n",
    "##### The normalized dataset had worse results in addition to requiring additional memory to store float64 values instead of int8. Raw data should be chosen from then on.\n",
    "#### Balanced versus RFS, PCA, SIFT\n",
    "##### All metrics resulting from the preprocessing techniques were worse than corresponding metrics in the non preprocessed balanced dataset. Thus, no preprocessing techniques would be utilized.\n",
    "#### Verdict:\n",
    "##### The non-normalized, balanced, non-preprocessed dataset has the best performance out of all iterations with an accuracy of 0.42."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c1855-2a5f-4cf5-b58c-391f32cb6963",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e89db7-7bfe-492c-89a7-4ce2b4dc4d96",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846943c2-9c3b-4e4f-915f-7bca076c2813",
   "metadata": {},
   "source": [
    "##### After concluding that a balanced dataset yielded the best performance and that the model itself was the 2nd most accurate, further hyperparameter tuning was needed to achieve a global maximum of performance. Five hyperparameters were tuned for the following runs: Dropout, L1 Regularization, L2 Regularization, Learning Rate, and Number of samples. A hyperparameter would only be considered if its accuracy was greater than the base model by more than 0.01. This threshold is to ensure that improvements are not due to changes in samplinga and are instead because the model genuinely improved.\n",
    "#### Base Model's Hyperparameters\n",
    "##### - Dropout: 0\n",
    "##### - L1: 0\n",
    "##### - L2: 0\n",
    "##### - Lr: 0.001\n",
    "##### - Sampling Size: 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de11d3-1987-4014-a3fa-6a90a093ce26",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a2552-f31d-4555-9635-3a78a411e5be",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr width = \"2000\">\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/L2Report.PNG\"/>\n",
    "            <figcaption>Fig.15 L2 hyperparameter tuning values</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/L1Report.PNG\" />\n",
    "            <figcaption>Fig.16 L1 hyperparameter tuning values</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/DropoutReport.PNG\" />\n",
    "            <figcaption>Fig.17 Dropout hyperparameter tuning values</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/LrReport.PNG\" />\n",
    "            <figcaption>Fig.18 Learning Rate hyperparameter tuning values</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7965b295-a7d9-44df-8ccd-75769ed706f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Based on hyperparameter tuning on all models, L1: 0.001 and Lr: 0.0001 were the best candidates since their accuracy was beyond 0.01 of the base accuracy of 0.485552."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68449cf6-6e1f-46a6-9276-5f6ad892716e",
   "metadata": {},
   "source": [
    "### Sample Size Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07377eae-70f7-4438-8a56-9881bf2e0452",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr width = \"2000\">\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/SampleAcc.PNG\"/>\n",
    "            <figcaption>Fig.27 Sample Size hyperparameter tuning versus accuracy</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/SampleLoss.PNG\" />\n",
    "            <figcaption>Fig.28 Sample Size hyperparameter tuning versus loss</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae66871-fa47-42f5-bfff-1c9e0bf2b604",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Sample size amounts were tested from 500 to 8000 in intervals of 500. It was found that 5000 had the highest accuracy. Like before, the model loss seems to inversely correlate with the model's accuracy. Although the model was most accurate on 5000 samples, we will be using the 8000 as using more data is best practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5031bbea-7987-4214-9170-1d8e38e3968f",
   "metadata": {},
   "source": [
    "### Post Hyperparameter testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6b5de5-1e0a-43fb-a213-b85733f0bde2",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr width = \"2000\">\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/8000TestRaceReport.PNG\"/>\n",
    "            <figcaption>Fig.30 Classification report for the race test set</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/8000TestReport.PNG\" />\n",
    "            <figcaption>Fig.31 Classification report on the age test set</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td width=\"500\">\n",
    "        <figure>\n",
    "            <img src=\"../Reports/figures/NeuralNetwork/HyperparameterTuning/8000TestGenderReport.PNG\" />\n",
    "            <figcaption>Fig.32 Classification report on the gender test set</figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a0465d-cad8-46d2-ba2c-6c05e6257eec",
   "metadata": {},
   "source": [
    "##### After some basic runs, it was discovered that LR: 0.0001 had the best validation accuracy at 0.61 and a test accuracy of 0.62. The classification report on age shows far better results than that of the control model. In addition, the race and gender model using the same neural network structure achieved better results than the dummy models. Thus, this model was to be considered satisfactory. The only thing left to do was to visualize the weakpoints of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3450dfaa-d9c4-44f2-8398-bb6bbcc54c0b",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dcf296-a336-4bc7-8a11-061e4129b37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d56a646-0ef1-4041-bb90-66d86613f4f4",
   "metadata": {},
   "source": [
    "# Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1a5b3-de40-4302-8501-3182bc7dcce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afc89917-1a55-414d-961f-9b0cf365f88a",
   "metadata": {},
   "source": [
    "# Bibliography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bb54d4-d684-437b-b17d-5f5da5249f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80c029bf-33fa-4643-8a92-a7e73c528f5f",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2490961-3d9a-40a7-a5cb-a2d913372784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
