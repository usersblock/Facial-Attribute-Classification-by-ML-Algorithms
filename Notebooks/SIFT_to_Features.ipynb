{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609a1995-e07b-4616-bc20-359ee9402d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..\\src\\data')\n",
    "import make_dataset_beta as md\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.vq import vq, kmeans\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a974695b-3458-46d0-817c-9c449addf326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Image Descriptors, which are a combination of points on an image and the description of surrounding pixels.\n",
    "\n",
    "def get_descriptors(nparrays,nfeatures):\n",
    "    sift = cv2.SIFT_create(nfeatures = nfeatures)\n",
    "    container = []\n",
    "    for i in nparrays:\n",
    "        img_bw = cv2.cvtColor(i, cv2.COLOR_BGR2GRAY)\n",
    "        keypoint,descriptor = sift.detectAndCompute(img_bw,None)\n",
    "        container.append([keypoint,descriptor])\n",
    "    return container\n",
    "\n",
    "#For a collection of image area descriptions, get the Kmeans of n clusters. This will be what future images are compared to.\n",
    "\n",
    "def get_vocab(descriptors,n):\n",
    "    descriptor_container = []\n",
    "    for i in descriptors:\n",
    "        if i[1] is None:\n",
    "            continue\n",
    "        for j in i[1]:\n",
    "            descriptor_container.append(j)\n",
    "    vocab = kmeans(descriptor_container,n)\n",
    "    return vocab\n",
    "\n",
    "# For each image, get its descriptors. For each descriptor, get the closest Kmean descriptor in vocab and add 1 to its index in a histogram.\n",
    "# Return a histogram per image. This histogram will be passed as a feature for modeling.\n",
    "\n",
    "def descriptor_to_vocab(nparrays,vocab):\n",
    "    sift = cv2.SIFT_create()\n",
    "    container = []\n",
    "    for i in nparrays:\n",
    "        img_bw = cv2.cvtColor(i, cv2.COLOR_BGR2GRAY)\n",
    "        keypoint,descriptor = sift.detectAndCompute(img_bw,None)\n",
    "        if descriptor is not None:\n",
    "            dist = cdist(descriptor,vocab[0],'euclidean')\n",
    "            bin_assignment = np.argmin(dist,axis = 1)\n",
    "        else:\n",
    "            bin_assignment = []\n",
    "        image_feats = np.zeros(len(vocab[0]))\n",
    "        for j in bin_assignment:\n",
    "            image_feats[j] += 1\n",
    "        container.append(image_feats)\n",
    "    return container\n",
    "\n",
    "# Normalizes histograms from images so that they may be used in ML inputs\n",
    "\n",
    "def normalize_histograms(histarray):\n",
    "    histarray = np.array(histarray)\n",
    "    feats_norm_div = np.linalg.norm(histarray,axis = 1)\n",
    "    for i in range(0,histarray.shape[0]):\n",
    "        divi = feats_norm_div[i]\n",
    "        if(divi == 0):\n",
    "            divi = 1\n",
    "        histarray[i] = histarray[i]/feats_norm_div[i]\n",
    "    return histarray\n",
    "\n",
    "# Pipeline for SIFT to histogram features per image\n",
    "# Returns a dataframe for training and testing datasets\n",
    "'''\n",
    "path - path to the image directory\n",
    "column - what type of label to return\n",
    "nvocab - how many vectors will appear in the descriptor 'vocabulary'\n",
    "test_size - percentage of images that will be used as tests\n",
    "random_state - seed for randomization\n",
    "n_features - For each image, return at most n descriptors\n",
    "'''\n",
    "\n",
    "def SIFT_path_to_Features(path,column,nvocab = 200,test_size = 0.33,random_state = 42,n_features = 100):\n",
    "    filedf = md.get_image_label_filepath_df('./Data/CroppedImages/')\n",
    "    X,y = md.get_labels_npimages(filedf,column)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    X_train_np = np.array(X_train)\n",
    "    X_test_np = np.array(X_test)\n",
    "    y_train_np = np.array(y_train)\n",
    "    y_test_np = np.array(y_test)\n",
    "    descriptors = get_descriptors(X_train_np[:,1],n_features)\n",
    "    vocab = get_vocab(descriptors[:100],nvocab)\n",
    "    histarraytrain = descriptor_to_vocab(X_train_np[:,1],vocab)\n",
    "    histarraytest = descriptor_to_vocab(X_test_np[:,1],vocab)\n",
    "    normalizehisttrain = normalize_histograms(histarraytrain)\n",
    "    normalizehisttest = normalize_histograms(histarraytest)\n",
    "    traindf = pd.DataFrame((X_train_np[:,0],normalizehisttrain,y_train_np))\n",
    "    testdf = pd.DataFrame((X_test_np[:,0],normalizehisttest,y_test_np))\n",
    "    traindf = traindf.transpose()\n",
    "    testdf = testdf.transpose()\n",
    "    return [traindf,testdf]\n",
    "\n",
    "# X[:,0] should be file names and X[:,1] should be nparrays of images\n",
    "\n",
    "def SIFT_nparray_to_Features(X_train,X_test,nvocab = 200,n_features = 100):\n",
    "    X_train_np = np.array(X_train)\n",
    "    X_test_np = np.array(X_test)\n",
    "    descriptors = get_descriptors(X_train_np,n_features)\n",
    "    vocab = get_vocab(descriptors[:100],nvocab)\n",
    "    histarraytrain = descriptor_to_vocab(X_train_np,vocab)\n",
    "    histarraytest = descriptor_to_vocab(X_test_np,vocab)\n",
    "    normalizehisttrain = normalize_histograms(histarraytrain)\n",
    "    normalizehisttest = normalize_histograms(histarraytest)\n",
    "    return [normalizehisttrain,normalizehisttest]\n",
    "\n",
    "def SIFT_np_to_Features(filenames,X,y,nvocab = 200,test_size = 0.33,random_state = 42,n_features = 100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    X_train_np = np.array(X_train)\n",
    "    X_test_np = np.array(X_test)\n",
    "    y_train_np = np.array(y_train)\n",
    "    y_test_np = np.array(y_test)\n",
    "    descriptors = get_descriptors(X_train_np,n_features)\n",
    "    vocab = get_vocab(descriptors[:100],nvocab)\n",
    "    histarraytrain = descriptor_to_vocab(X_train_np,vocab)\n",
    "    histarraytest = descriptor_to_vocab(X_test_np,vocab)\n",
    "    normalizehisttrain = normalize_histograms(histarraytrain)\n",
    "    normalizehisttest = normalize_histograms(histarraytest)\n",
    "    traindf = pd.DataFrame((filenames,normalizehisttrain,y_train_np))\n",
    "    testdf = pd.DataFrame((filenames,normalizehisttest,y_test_np))\n",
    "    traindf = traindf.transpose()\n",
    "    testdf = testdf.transpose()\n",
    "    return [traindf,testdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3149759-1499-4bd8-b8ee-f782182bf512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
