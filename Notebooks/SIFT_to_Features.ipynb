{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609a1995-e07b-4616-bc20-359ee9402d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import make_dataset as md\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.vq import vq, kmeans\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a974695b-3458-46d0-817c-9c449addf326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Image Descriptors, which are a combination of points on an image and the description of surrounding pixels.\n",
    "\n",
    "def get_descriptors(nparrays,nfeatures):\n",
    "    sift = cv2.SIFT_create(nfeatures = nfeatures)\n",
    "    container = []\n",
    "    for i in nparrays:\n",
    "        img_bw = cv2.cvtColor(i, cv2.COLOR_BGR2GRAY)\n",
    "        keypoint,descriptor = sift.detectAndCompute(img_bw,None)\n",
    "        container.append([keypoint,descriptor])\n",
    "    return container\n",
    "\n",
    "#For a collection of image area descriptions, get the Kmeans of n clusters. This will be what future images are compared to.\n",
    "\n",
    "def get_vocab(descriptors,n):\n",
    "    descriptor_container = []\n",
    "    for i in descriptors:\n",
    "        if i[1] is None:\n",
    "            continue\n",
    "        for j in i[1]:\n",
    "            descriptor_container.append(j)\n",
    "    vocab = kmeans(descriptor_container,n)\n",
    "    return vocab\n",
    "\n",
    "# For each image, get its descriptors. For each descriptor, get the closest Kmean descriptor in vocab and add 1 to its index in a histogram.\n",
    "# Return a histogram per image. This histogram will be passed as a feature for modeling.\n",
    "\n",
    "def descriptor_to_vocab(nparrays,vocab):\n",
    "    sift = cv2.SIFT_create()\n",
    "    container = []\n",
    "    for i in nparrays:\n",
    "        img_bw = cv2.cvtColor(i, cv2.COLOR_BGR2GRAY)\n",
    "        keypoint,descriptor = sift.detectAndCompute(img_bw,None)\n",
    "        if descriptor is not None:\n",
    "            dist = cdist(descriptor,vocab[0],'euclidean')\n",
    "            bin_assignment = np.argmin(dist,axis = 1)\n",
    "        else:\n",
    "            bin_assignment = []\n",
    "        image_feats = np.zeros(len(vocab[0]))\n",
    "        for j in bin_assignment:\n",
    "            image_feats[j] += 1\n",
    "        container.append(image_feats)\n",
    "    return container\n",
    "\n",
    "# Normalizes histograms from images so that they may be used in ML inputs\n",
    "\n",
    "def normalize_histograms(histarray):\n",
    "    histarray = np.array(histarray)\n",
    "    feats_norm_div = np.linalg.norm(histarray,axis = 1)\n",
    "    for i in range(0,histarray.shape[0]):\n",
    "        histarray[i] = histarray[i]/feats_norm_div[i]\n",
    "    return histarray\n",
    "\n",
    "# Pipeline for SIFT to histogram features per image\n",
    "'''\n",
    "path - path to the image directory\n",
    "column - what type of label to return\n",
    "nvocab - how many vectors will appear in the descriptor 'vocabulary'\n",
    "test_size - percentage of images that will be used as tests\n",
    "random_state - seed for randomization\n",
    "n_features - For each image, return at most n descriptors\n",
    "'''\n",
    "\n",
    "def SIFT_to_Features(path,column,nvocab = 200,test_size = 0.33,random_state = 42,n_features = 100):\n",
    "    filedf = md.get_image_label_filepath_df('./Data/CroppedImages/')\n",
    "    X,y = md.get_labels_npimages(filedf,column)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    X_train_np = np.array(X_train)\n",
    "    X_test_np = np.array(X_test)\n",
    "    y_train_np = np.array(y_train)\n",
    "    y_test_np = np.array(y_test)\n",
    "    descriptors = get_descriptors(X_train_np[:,1],n_features)\n",
    "    vocab = get_vocab(descriptors[:100],nvocab)\n",
    "    # Get histogram values from X_train descriptors\n",
    "    histarraytrain = descriptor_to_vocab(X_train_np[:,1],vocab)\n",
    "    # Get histogram values from X_test descriptors\n",
    "    histarraytest = descriptor_to_vocab(X_test_np[:,1],vocab)\n",
    "    # Normalize train histograms\n",
    "    normalizehisttrain = normalize_histograms(histarraytrain)\n",
    "    # Normalize test histograms\n",
    "    normalizehisttest = normalize_histograms(histarraytest)\n",
    "    return [(X_train_np[:,0],normalizehisttrain,y_train_np),(X_test_np[:,0],normalizehisttest,y_test_np)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b4152cf-f104-4684-9444-76f000ef7c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thanh\\AppData\\Local\\Temp\\ipykernel_10160\\3187525907.py:56: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train_np = np.array(X_train)\n",
      "C:\\Users\\thanh\\AppData\\Local\\Temp\\ipykernel_10160\\3187525907.py:57: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_test_np = np.array(X_test)\n",
      "C:\\Users\\thanh\\AppData\\Local\\Temp\\ipykernel_10160\\3187525907.py:49: RuntimeWarning: invalid value encountered in true_divide\n",
      "  histarray[i] = histarray[i]/feats_norm_div[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 18min 14s\n",
      "Wall time: 4min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train,test = SIFT_to_Features('./Data/CroppedImages/','race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e44fe581-87c1-4665-b2a6-a3e05ba8dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.DataFrame(train)\n",
    "testdf = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7218d471-70a7-4765-b9ae-739d35787e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = traindf.transpose()\n",
    "testdf = testdf.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7dd7814-c657-4dea-bc35-70b6e83e92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.to_csv('filename',index=False)\n",
    "testdf.to_csv('filename',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
